\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}
\usepackage{palatino}
\usepackage[strict]{changepage}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{csquotes}
\usepackage{verbatim}
\usepackage[cache=false]{minted}
\usepackage[ruled,vlined]{algorithm2e}

\newenvironment{changemargin}[2]{%
  \begin{list}{}{%
      \setlength{\topsep}{0pt}%
      \setlength{\leftmargin}{#1}%
      \setlength{\rightmargin}{#2}%
      \setlength{\listparindent}{\parindent}%
      \setlength{\itemindent}{\parindent}%
      \setlength{\parsep}{\parskip}%
    }%
  \item[]}{\end{list}}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usemintedstyle{default}
\setminted{
  linenos=true,
  breaklines=true,
  fontsize=\small,
  frame=single,
}
% \usemintedstyle{fruity}

\title{Community detection in networks}
\author{Carlos Requena LÃ³pez}

%% Fancy layout
\pagestyle{fancy}
\lhead{INFO-F521}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{Page \thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}


%%% --- %%% --- DOCUMENT START --- %%% --- %%%
\begin{document}
\thispagestyle{fancy}
\maketitle
\thispagestyle{fancy}

\section{Introduction}

The topic of this assignment is community detection in networks by
means of hierarchical clustering. Informally, network communities can
be seen as dense (highly connected) subgraphs that are connected
between each other sparsely, only by few arcs or edges.

Hierarchical clustering (HC) refers to the technique of grouping
together elements that share some sort of similarity and separating
those that do not. HC can be \emph{divisive}, where elements start in
one cluster and are removed gradually or \emph{agglomerative}, where
every unit is its own cluster and are merged step by step. Only
\emph{agglomerative} clustering will be considered in this assignment:
in particular single-linkage and complete-linkage clustering.

\section{Analysis and expectations}

\subsection{Measure of similarity}

Let $ G = (V, E) $ be a simple, unweighted, undirected graph. Let $A$
be the adjacency matrix of $G$, defined as follows:

\[
  A_{ij} =
  \begin{cases}
    1 & \quad \text{if } (i,j) \in E\\
    0 & \quad \text{otherwise }
  \end{cases}
\]

We will use this adjacency matrix as the only source of information to
determine the similarity between vertices in the graph, but there
could be plenty of other sources depending on the application
domain. For example, when analysing text files, the similarity measure
could be the amount of equal significant words each one contains, or
in the case of aminoacids, the number of appearances at a certain
location.

Here, however, we compute the mean and variance between the rows of
the adjacency matrix as follows (if $A$ was not symmetric or $G$
undirected, it would have to be computed for rows \emph{and} columns):

$$ \mu_i = \frac{1}{n} \sum_{j=1}^{n} A_{ij} $$

$$ \sigma^2 = \frac{1}{n} \sum_{j=1}^{n} (A_{ij} - \mu_i)^2 $$

The similarity measure in this case is the Pearson product-moment
correlation coefficient:

$$ x_{ij} = \frac{\frac{1}{n}\sum_{k=1}^{n}(A_{ik} -
  \mu_i)(A_{jk}-\mu_j)}{\sigma_i\sigma_j} $$

All diagonal elements are excluded from these calculations
\cite[p.~369]{socionetwork}. The value $x_{ij}$ can range from -1 to
1, and it gives a measure of how structurally similar two vertices
are. Two vertices do not need to be adjacent for this quantity to be
significant, although they usually are.

\subsection{Hierarchical clustering}

Once the similarity measures are set, they serve as a base to
implement agglomerative HC. By observing the set of partitions (the
number of partitions is the number of elements at step 0), we can
define the function to maximise when merging two of them. This is
repeated until there is only one partition (containing all the
elements) left.

Algorithms for HC do not require the size of the partitions of the
number of partitions to be specified. Obviously, the output of the
program - a set containing all vertices - is not really meaningful;
that is why the process is captured in a data structure called
\emph{dendogram}. This one can be \emph{cut} horizontally to inspect
the clusters at the cut level.

\subsubsection{Pseudocode of single linkage and complete linkage
  algorithms}

The pseudocodes for basic single linkage and complete linkage
clustering are given in algorithms \ref{slink} and \ref{clink}.

\begin{algorithm}[h]
  \SetAlgoLined
  \KwIn{An undirected graph G}
  \KwOut{The tree of clusters}

  \nl Compute the proximity (similarity) matrix X for the given G.\;
  \nl Place every vertex in its own set.\;
  \nl Find the two most similar elements in different sets and merge them.\;
  \nl Remove one of the entries in X and update the other by choosing the highest
  values possible.\;
  \nl Append merge information to tree to keep the history of events.\;
  \nl \While{X \textrm{has shape} > 1x1}{
    \nl Repeat lines 3 through 5\;
  }
  \caption{\bf SLOW\_S-LINK}
  \label{slink}
\end{algorithm}

\begin{algorithm}[h]
  \SetAlgoLined
  \KwIn{An undirected graph G}
  \KwOut{The tree of clusters}

  \nl Compute the proximity (similarity) matrix X for the given G.\;
  \nl Place every vertex in its own set.\;
  \nl Find the two most similar elements in different sets and merge them.\;
  \nl Remove one of the entries in X and update the other one by choosing the lowest
  values possible.\;
  \nl Append merge information to tree to keep the history of events.\;
  \nl \While{X \textrm{has shape} > 1x1}{
    \nl Repeat lines 3 through 5\;
  }
  \caption{\bf SLOW\_C-LINK}
  \label{clink}
\end{algorithm}

The last step of the algorithm, which is keeping information about
the merges done, is useful to plot the dendogram or recreate the
merge history and be able to ``cut'' the tree at a certain point to
obtain the partitions.

\subsubsection{Computational complexity}

For both cases, the complexity of the algorithm is
$\mathcal{O}(n^3)$: computing the similarity matrix (or distance
information) takes $\mathcal{O}(nm)$ \cite{newman}, where $n$ is the
number of vertices and $m$ is the number of edges. We need to check
the minimum element in an array of size $n^2$, so by the adversary
argument, this takes $\Omega (n^2)$, and this check has to be
done a maximum of $n$ times. Therefore, the upper bound for the
naive algorithm is $\mathcal{O}(n^3)$.

As a side note, the library \texttt{SciPy} for Python, implements an
$\mathcal{O}(n^2)$ algorithm for single-linkage and
complete-linkage, based on minimum spanning tree and
nearest-neighbour chain respectively.

\subsubsection{Difference between single link and complete link}

On the one hand, single-linkage merges two clusters or partitions by
retaining the maximum similarity (or minimum distance) between the
two. The main formula behind the concept of single linkage is:

$$ \mathrm{max}\{x_{i,j} : i \in P_a, j \in P_b\} $$

where ${P_a}$ and $P_b$ are two partitions of $V$.

This gives rise to the \emph{chaining phenomenon}, e.g.: if
there is a very ``powerful'' node with high similarity with a lot of
other nodes, the latter will be chained successively to the first
one since it is the only one that counts in that cluster.

On the other hand, complete linkage remembers the ``weakest'' or more
distant element there is when merging two clusters. This results in
more balanced, similar sized clusters. Usually, this is what we are
looking for. The function used is:

$$ \mathrm{min}\{x_{i,j} : i \in P_a, j \in P_b\} $$

A third approach not discussed here, \emph{average linkage}, does not
consider a single vertex (the one with lowest or highest scores) per
partition. Instead, it computes an average of similarities among the
members of the newly created partition, and sets the score of the
member to that average.

Figures \ref{fig:slink-vs-clink} and \ref{fig:slink-vs-clink2} show an
example for the same random graph, for single-linkage and
complete-linkage at two different cuts in the tree.

\begin{figure}
  \centering
  \begin{subfigure}[h]{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/slink-200-160.png}
    \caption{Single linkage}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/clink-200-160.png}
    \caption{Complete linkage}
  \end{subfigure}
  \caption{Random graph. Tree cut at step 160. For $n = 200$}
  \label{fig:slink-vs-clink}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[h]{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/slink-200-180.png}
    \caption{Single linkage}
  \end{subfigure}
  \begin{subfigure}[h]{0.49\textwidth}
    \includegraphics[width=\linewidth]{img/clink-200-180.png}
    \caption{Complete linkage}
  \end{subfigure}
  \caption{Random graph. Tree cut at step 180. For $n = 200$}
  \label{fig:slink-vs-clink2}
\end{figure}

Every colour represents a different partition.

\subsubsection{Optimality of solution at each step}

The main disadvantage of complete linkage is that it considers
degenerate points (isolated vertices in our case) to have more
importance than what they really do.

\subsubsection{What happens in the case of a tie?}

Several approaches exist to deal with the non-uniqueness problem of
similarities in the data set \cite{nonunique}:

\begin{itemize}
\item Arbitrarily choosing between the elements in the tie.
\item Deterministically choosing between the elements in the tie
  (the element with smaller index for example).
\item Merge all tied elements at the same time.
\end{itemize}


\section{Implementation and results}

A graph $ G $ is given as an adjacency matrix, and from it, the
similarity matrix can be obtained.

We can then proceed to do some tests on different networks to see how
both methods perform. We will cut the tree at a late stage, to obtain
decently sized clusters. We can then check if the clusters correspond
to dense areas of the graph that have sparse connections to other
components.

\section{Conclusion}



% \begin{figure}[ht!]
%   \centering
%   \includegraphics[width=1\textwidth]{some_graphic.png}
%   \caption{}
%   \label{fig:fig1}
% \end{figure}

\bibliographystyle{ieeetr}
\bibliography{main}
\nocite{*}

\appendix
\section{Appendix - code listing}

\inputminted[label=main.py]{python}{../src/main.py}
\hfill
\inputminted[label=measures.py]{python}{../src/measures.py}
\hfill
\inputminted[label=clink.py]{python}{../src/link.py}

\end{document}
